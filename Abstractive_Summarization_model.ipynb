{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76b604e",
   "metadata": {},
   "source": [
    "### How the model works\n",
    "Documentation is provided for each function to make the model more understandable.Refer to it.\n",
    "for more information refer to pages ( - ) in survey.pdf( ) -Will be updated soon\n",
    "\n",
    "This notebook is an compilation of every function created to implement the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preImport(k2tpath):\n",
    "        global clasr,happy_tt,args\n",
    "        import pickle\n",
    "        f=open('finalized_model.sav', 'rb')\n",
    "        clasr = pickle.load(f)\n",
    "        f.close()\n",
    "        from gramformer import Gramformer\n",
    "        global gf \n",
    "        gf=Gramformer(models = 1, use_gpu=False)\n",
    "        import torch\n",
    "        def set_seed(seed):\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "        set_seed(1212)\n",
    "        global model\n",
    "        from keytotext import trainer\n",
    "        model=trainer()\n",
    "        model.load_model(k2tpath, use_gpu=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fa813",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7f812",
   "metadata": {},
   "source": [
    "### 1)Cleaning and dataframe generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9d472",
   "metadata": {},
   "source": [
    "**clean(text,removestop)**\n",
    "\n",
    "returns a cleaned text of the article/summary, expanding contractions such as ain't->am not, removing details of publishing date, published websites,stopwords(if passed text is an article)..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text,removestop=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    import contractions\n",
    "    stops = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    tmp = []\n",
    "    #expanding contractions\n",
    "    for word in text:\n",
    "        if((not removestop) or (word not in stops)):\n",
    "            tmp.append(contractions.fix(word)) \n",
    "    text = ' '.join(tmp)\n",
    "    \n",
    "    #Substituting unncessary words using re.sub\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text=re.sub(' est ','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c79ba",
   "metadata": {},
   "source": [
    "**sentCreator(text)**\n",
    "\n",
    "when passed a string text, returns sentences of the text as a single string. the produced string can be split into different sentences by using .split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef989b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentCreator(text):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences=\"\"\n",
    "    for sent in sent_tokenize(text):\n",
    "        #less than 6 words in main article indicates very less important sentence, we can effectively ignore them\n",
    "        if(len(sent.split())>5):\n",
    "            sentences+=sent\n",
    "    return sentences\n",
    "    #Sentences generated from tokenize end with . , we can use this info to just create list of sentences by using .split(.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f7708",
   "metadata": {},
   "source": [
    "**createCleaned(filename/path,training=True)**\n",
    "\n",
    "when the path of the file/filename (.csv file) is passed, createCleaned will save a csv file titled \"Cleaned.csv\". \n",
    "\n",
    "**Note** Formats required and produced under training and testing:\n",
    "\n",
    "**+** Under training: \n",
    "\n",
    "passed dataframe:  |--unused column--|--Articles--|--Summaries--|\n",
    "\n",
    "produced dataframe: |--Index--|--Articles--|--Summaries--|--Sentences--|\n",
    "\n",
    "**+** Under prediction/testing: \n",
    "\n",
    "passed dataframe:  |--unused column--|--Articles--|\n",
    "\n",
    "produced dataframe: |--Index--|--Articles--|--Sentences--|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCleaned(filename,training=True,noFile=False):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        df=pd.DataFrame()\n",
    "        if(noFile):\n",
    "            df=filename\n",
    "            article=df.iloc[:,1].values\n",
    "        else:\n",
    "            df=pd.read_csv(filename)\n",
    "            article=df.iloc[:,1].values\n",
    "        if(training):\n",
    "            train_summary=df.iloc[:,2].values\n",
    "        ta=[]\n",
    "        ts=[]\n",
    "        sen=[]\n",
    "        fiv_percent=int(len(article)*0.05)+1\n",
    "        for x in range(0,len(article)):\n",
    "            ta.append(clean(article[x]))\n",
    "            sen.append(sentCreator(article[x]).lower())\n",
    "            if(training):\n",
    "                ts.append(clean(train_summary[x],False))\n",
    "            if(x%fiv_percent==0):\n",
    "                #to track progress\n",
    "                print(x,\" out of \",len(article),\" parsed\")\n",
    "        cleaned=pd.DataFrame()\n",
    "        cleaned[\"Articles\"]=ta\n",
    "        if(training):\n",
    "            cleaned[\"Summaries\"]=ts\n",
    "        cleaned[\"Sentences\"]=sen\n",
    "        cleaned.drop_duplicates(inplace=True)\n",
    "        cleaned.dropna(inplace=True)\n",
    "        length=len(cleaned)\n",
    "        index=[x for x in range(0,length)]\n",
    "        cleaned[\"Index\"]=index\n",
    "        cleaned.set_index(\"Index\",inplace=True)\n",
    "        cleaned.to_csv(\"Cleaned.csv\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa308c8c",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceaa924",
   "metadata": {},
   "source": [
    "### 2)Extracting features and creating a supervised learning model, to get probabilities of a word being present in summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded12b5b",
   "metadata": {},
   "source": [
    "**importCleaned(filename/path)**\n",
    "\n",
    "returns dataframe under passed filepath, can be used to retrieve dataframe from createCleaned function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importCleaned(filename):\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(filename)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a65ee7",
   "metadata": {},
   "source": [
    "**absScore(text)**\n",
    "\n",
    "given an input text, absScore returns a \"word\":AbsoluteScore dictionary. Absolutescore(x) denotes the Probability of the picked word being x, when you select a random word from the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absScore(text):\n",
    "    words_freq={}\n",
    "    words=text.split()\n",
    "    total=0\n",
    "    for w in words:\n",
    "        total+=1\n",
    "        try:\n",
    "            words_freq[w]+=1\n",
    "        except:\n",
    "            words_freq[w]=1  \n",
    "    for i in words_freq.keys():\n",
    "        words_freq[i]/=total\n",
    "    return words_freq    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cdc1c",
   "metadata": {},
   "source": [
    "**sentScore(words,sentences)**\n",
    "\n",
    "given the inputs: word-AbsoluteScore dictionary  &  string sentences to which these words belong to(sentences end with \".\" -ie format produced from sentCreator function),sentScore will return a dictionary of form sentence_number-sentenceScore.\n",
    "\n",
    "sentenceScore (or) Score of a sentence is the average word probability(absScore) in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentScore(words,sentences):\n",
    "    sents=sentences.split(\".\")\n",
    "    sentscore={}\n",
    "    keys=words.keys()\n",
    "    s=0\n",
    "    for i in range(0,len(sents)):\n",
    "        score=0\n",
    "        itrsent=sents[i].split()\n",
    "        itrsent_indict=[]\n",
    "        for j in itrsent:\n",
    "            if(j in keys):\n",
    "                itrsent_indict.append(1)\n",
    "            else:\n",
    "                itrsent_indict.append(0)\n",
    "        n=0\n",
    "        for j in range(0,len(itrsent)):\n",
    "            n+=1\n",
    "            if(itrsent_indict[j]):\n",
    "                score+=words[itrsent[j]]  \n",
    "        if(n==0):\n",
    "            sentscore[i]=0\n",
    "        else:\n",
    "            score/=n\n",
    "            sentscore[i]=score\n",
    "        s+=score\n",
    "    for i in sentscore.keys():\n",
    "        sentscore[i]/=s\n",
    "    return sentscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cde36",
   "metadata": {},
   "source": [
    "**inFirst(words,sentences)**\n",
    "\n",
    "given the inputs: word-AbsoluteScore dictionary  &  string sentences to which these words belong to(each sentence ends with \".\" -ie format produced from sentCreator function) , inFirst will return a dictionary of form word-inFirstsentence, where inFirstsentence takes the value 0(word not in first sentence) or 1 (word in first sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inFirst(words,sentences):\n",
    "    inFirst={}\n",
    "    sent=(sentences.split(\".\"))[0]\n",
    "    for i in words.keys():\n",
    "        if(i in sent):\n",
    "            inFirst[i]=1\n",
    "        else:\n",
    "            inFirst[i]=0\n",
    "    return inFirst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ca843",
   "metadata": {},
   "source": [
    "**mostProminent(words,sentences,senrank)**\n",
    "\n",
    "given the inputs: word-AbsoluteScore dictionary , string sentences to which these words belong to, and sentence-sentScore dictionary, mostProminent will return a word-mostProminentSentence dictionary . mostProminent sentence is the sentence in which the corresponding word occurs the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostProminent(words,sentences,senrank):\n",
    "    sent=sentences.split(\".\")\n",
    "    mostProminent={}\n",
    "    for i in words.keys():\n",
    "        mps=0\n",
    "        counts=0\n",
    "        for j in range(0,len(sent)):\n",
    "            c=sent[j].count(i)\n",
    "            if(c>counts or ((c==counts)and senrank[j]>senrank[mps])):\n",
    "                counts=c\n",
    "                mps=j\n",
    "        mostProminent[i]=mps\n",
    "    return mostProminent  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2671444f",
   "metadata": {},
   "source": [
    "**relScore(absscore,senscore,mostprominent)**\n",
    "\n",
    "given the inputs word-AbsoluteScore dictionary , sentence-sentScore dictionary and word-mostProminent dictionary, relScore will return a word-relative_score dictionary. where relativeScore of word w is given by :absScore(word)* sentScore(most prominent sentence(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relScore(absscore,senscore,mostprominent):\n",
    "    relscore={}\n",
    "    for i in absscore.keys():\n",
    "        relscore[i]=(absscore[i]*senscore[mostprominent[i]])\n",
    "    return relscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befae34",
   "metadata": {},
   "source": [
    "**inSummary(absscore,summary,training=True)**\n",
    "\n",
    "inSummary takes in 2 inputs- word-Absoloute score dictionary, the corresponding summary to which the word dictionary belongs to. inSummary acts differently based on input given to the parameter training(True/False)\n",
    "\n",
    "**+** training is True(default case):\n",
    "\n",
    "will return a word-inSummary dictionary. inSummary produces values 0(if word is not in summary) or 1(if word is in summary)\n",
    "\n",
    "**+** training is False:\n",
    "\n",
    "will return a word-0 dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inSummary(absscore,summary,training=True):\n",
    "    inSummary={}\n",
    "    if(training):\n",
    "        for i in absscore.keys():\n",
    "            if i in summary:\n",
    "                inSummary[i]=1\n",
    "            else:\n",
    "                inSummary[i]=0\n",
    "    else:\n",
    "        for i in absscore.keys():\n",
    "            inSummary[i]=0\n",
    "    return inSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c10c3",
   "metadata": {},
   "source": [
    "**summLen(absscore,summary=\" \",default_len=200,training=True)**\n",
    "\n",
    "\n",
    "summLen takes in 4 inputs- word-Absoloute score dictionary, the corresponding summary to which the word dictionary belongs to( by default-\" \")and length( by default 200). summLen acts differently based on input given to the parameter training (either True/False).\n",
    "\n",
    "**+** training is True(default case):\n",
    "\n",
    "will return a word-summLen dictionary. summLen produces wordcount of the respective summary , the word belongs to.\n",
    "\n",
    "**+** training is False:\n",
    "\n",
    "will return a word-default_len dictionary. based on value passed to default_len.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summLen(absscore,summary=\" \",default_len=200,training=True):\n",
    "    summlen={}\n",
    "    if(training):\n",
    "        length=len(summary.split())\n",
    "        for i in absscore.keys():\n",
    "            summlen[i]=length\n",
    "    else:\n",
    "        for i in absscore.keys():\n",
    "            summlen[i]=default_len\n",
    "    return summlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8871b4",
   "metadata": {},
   "source": [
    "**dfCreator(df,start,stop,training=True,length=200)**\n",
    "\n",
    "dfCreator acts differently based on values passes to training(T/F).\n",
    "\n",
    "**+** training is True(default case):\n",
    "\n",
    "will return a dataframe with the structure\n",
    "\n",
    "*|  Index  |  Words  |  Absolute score  |  Relative score  |  inFirst sentence  |  inSummary  |  Summary Length  |  Summary  |*\n",
    "\n",
    "Where column 2(abs score) to 7 are obtained by passing it to respective functions declared above with value of training being True\n",
    "\n",
    "**+** training is False:\n",
    "\n",
    "will return a dataframe with structure\n",
    "\n",
    "*|  Index  |  Words  |  Absolute score  |  Relative score  |  inFirst sentence  |  inSummary  |  Summary Length  |*\n",
    "\n",
    "Where column 2(abs score) to 7 are obtained by passing it to respective functions declared above with value of training being False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d024487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCreator(df,start,stop,training=True,length=200):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    indofsum=3\n",
    "    if(not training):\n",
    "        indofsum=2\n",
    "    absscore=[]\n",
    "    for i in range(start,stop):\n",
    "        absscore.append(absScore(df.iloc[i,1]))\n",
    "    sentscore=[]\n",
    "    for i in range(start,stop):\n",
    "        sentscore.append(sentScore(absscore[i-start],df.iloc[i,indofsum]))\n",
    "    infirst=[]\n",
    "    for i in range(start,stop):\n",
    "        infirst.append(inFirst(absscore[i-start],df.iloc[i,indofsum]))\n",
    "    mp=[]\n",
    "    for i in range(start,stop):\n",
    "        mp.append(mostProminent(absscore[i-start],df.iloc[i,indofsum],sentscore[i-start]))\n",
    "    relscore=[]\n",
    "    for i in range(start,stop):\n",
    "        relscore.append(relScore(absscore[i-start],sentscore[i-start],mp[i-start]))\n",
    "    insum=[]\n",
    "    if(training):\n",
    "        for i in range(start,stop):\n",
    "            insum.append(inSummary(absscore[i-start],df.iloc[i,2]))\n",
    "    else:\n",
    "        for i in range(start,stop):\n",
    "            insum.append(inSummary(absscore[i-start],0,False))\n",
    "    summlen=[]\n",
    "    if(training):\n",
    "        for i in range(start,stop):\n",
    "            summlen.append(summLen(absscore[i-start],df.iloc[i,2]))\n",
    "    else:\n",
    "        for i in range(start,stop):\n",
    "            summlen.append(summLen(absscore[i-start],default_len=length,training=False))\n",
    "        \n",
    "    final=[]\n",
    "    summaries=df.iloc[:,-2].values\n",
    "    summforkey=[]\n",
    "    index=[]\n",
    "    for i in range(0,len(relscore)):\n",
    "        keys=absscore[i].keys()\n",
    "        #structure of final: [word,[feautures]]\n",
    "        for j in keys:\n",
    "            if(training):\n",
    "                summforkey.append(summaries[start+i])\n",
    "            index.append(i)\n",
    "            currkey=j\n",
    "            feature=[absscore[i][currkey],relscore[i][currkey],infirst[i][currkey],insum[i][currkey],summlen[i][currkey]]\n",
    "            final.append([currkey,feature,])\n",
    "    words=[]\n",
    "    abscore=[]\n",
    "    relscore=[]\n",
    "    infirst=[]\n",
    "    insum=[]\n",
    "    summlen=[]\n",
    "    for i in range(0,len(final)):\n",
    "        words.append(final[i][0])\n",
    "        abscore.append(final[i][1][0])\n",
    "        relscore.append(final[i][1][1])\n",
    "        infirst.append(final[i][1][2])\n",
    "        insum.append(final[i][1][3])\n",
    "        summlen.append(final[i][1][4])\n",
    "    newdf=pd.DataFrame()\n",
    "    newdf[\"Index\"]=index\n",
    "    newdf[\"Words\"]=words\n",
    "    newdf[\"Absolute score\"]=abscore\n",
    "    newdf[\"Relative score\"]=relscore\n",
    "    newdf[\"In First sentence\"]=infirst\n",
    "    newdf[\"In Summary\"]=insum\n",
    "    newdf[\"Summary length\"]=summlen\n",
    "    if(training):\n",
    "        newdf[\"Summary\"]=summforkey\n",
    "    return newdf\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475383a",
   "metadata": {},
   "source": [
    "**NOTE** it is advised to run the function produceMLModel on collab/kaggle notebooks as for bigger dataset chances of getting Memoryrunouterror is high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ed6a8",
   "metadata": {},
   "source": [
    "**produceMLModel(df,start,stop)**\n",
    "\n",
    "Produces a ML model which transforms and trains on the cleaned dataset df(cleaned via createCleaned) and outputs a RandomForest model saved as *finalized_model.sav* under the current directory. it is able to classify/predict probabilities of a word being in the summary given the features engineered using above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceMLModel(df,start,stop):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    traindf=dfCreator(df,start,stop)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    x=traindf.iloc[:,[2,3,4,6]].values\n",
    "    y=traindf.iloc[:,5].values\n",
    "    clasr=RandomForestClassifier(n_estimators = 100, criterion = 'entropy',class_weight={0:0.95,1:0.05})\n",
    "    clasr.fit(x,y)\n",
    "    import pickle\n",
    "    filename = 'finalized_model.sav'\n",
    "    f=open(filename, 'wb')\n",
    "    pickle.dump(clasr, f)\n",
    "    f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5503c8",
   "metadata": {},
   "source": [
    "### 3)Generating sentences from set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a59828",
   "metadata": {},
   "source": [
    "**generateSet(df,start,stop,training=True,length=200,save_name=\"Keyword-text.csv\")**\n",
    "\n",
    "generates the best set of words for a given summary length( if testing ) , fixed summary length( taken from predefined summary during training) . more than several articles can be passed at once. produces a dataframe which has the structure:\n",
    "\n",
    "*|    keywords    |    text    |* (if training. text is the corresponding summary of the given keywords)\n",
    "\n",
    "*|    keywords    |    text    |* (if testing. Nan values for text, as it is not available)\n",
    "\n",
    "the dataframe is stored as \"Keyword-text.csv\" by default.\n",
    "\n",
    "if save_name is passed, it is stored as the string passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348172eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSet(df,start=0,stop=1,training=True,length=300,save_name=\"Keyword-text.csv\",new=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    if(new):\n",
    "        global clasr\n",
    "        import pickle\n",
    "        f=open('finalized_model.sav', 'rb')\n",
    "        clasr = pickle.load(f)\n",
    "    df=dfCreator(df,start,stop,training,length)\n",
    "    if(not training):\n",
    "        feauts=df.iloc[:,[2,3,4,6]].values\n",
    "        probas=clasr.predict_proba(feauts)[:,1]\n",
    "        df[\"Probability\"]=probas\n",
    "        df.drop(\"In Summary\",axis=1,inplace=True)\n",
    "        df.sort_values(by = ['Index', 'Probability'],ascending=False)\n",
    "    else:\n",
    "        df.drop(\"Summary length\",axis=1,inplace=True)\n",
    "    df.drop(\"Absolute score\",axis=1,inplace=True)\n",
    "    df.drop(\"Relative score\",axis=1,inplace=True)\n",
    "    df.drop(\"In First sentence\",axis=1,inplace=True)\n",
    "    f=open('finalized_model.sav', 'rb')\n",
    "    \n",
    "    #training: Index words inSummary summary\n",
    "    #test: Index words Summarylength Summary probability(in descending denoting high proba)\n",
    "    L=df.iloc[:,:].values\n",
    "    keywords=[]\n",
    "    summaries=[]\n",
    "    i=0\n",
    "    j=0\n",
    "    while( i< len(L)):\n",
    "        indexkey=L[i][0]\n",
    "        j=i\n",
    "        s=\"\"\n",
    "        length=lencovered=0\n",
    "        if(training):\n",
    "            while(j<len(L) and L[j][0]==indexkey):\n",
    "                if(L[j][2]==1):\n",
    "                    s+=(L[j][1]+\" \")\n",
    "                j+=1\n",
    "            i=j\n",
    "            summaries.append(L[j-1][3])\n",
    "            keywords.append(s)\n",
    "        if(not training):\n",
    "            length=int(L[i][2]*0.7)\n",
    "            while((j<len(L)) and (L[j][0]==indexkey) and lencovered<length):\n",
    "                lencovered+=1\n",
    "                s+=(L[j][1]+\" \")\n",
    "                j+=1\n",
    "            summaries.append(\" \")\n",
    "            keywords.append(s)\n",
    "            i=j\n",
    "    keyword_textDf=pd.DataFrame()\n",
    "    keyword_textDf[\"keywords\"]=keywords\n",
    "    keyword_textDf[\"text\"]=summaries\n",
    "    keyword_textDf.to_csv(save_name)\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb09a5",
   "metadata": {},
   "source": [
    "**Note** It is heavily recommended to use trainK2T function ONLY on kaggle/collab notebook as high amount of gpu processing power is required.\n",
    "\n",
    "### Important: after installing keytotext package, go to their directory , open trainer.py and search for progress_bar_refresh_rate (use Ctrl+f) . remove that single parameter alone. \n",
    "\n",
    "Further info for editing from notebook: https://stackoverflow.com/a/30250285/18432853"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a78a9",
   "metadata": {},
   "source": [
    "**trainK2T(traindf,testdf,model_=\"t5-small\")**\n",
    "\n",
    "trainK2T gets the train and test dataframes created from generateSet to train the KeyTotext model on the specified transformers model_. The model is saved under current directory under the model folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e993b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainK2T(traindf,testdf,model_=\"t5-small\"):\n",
    "    import pandas as pd\n",
    "    test_df = pd.read_csv(testdf)\n",
    "    train_df=pd.read_csv(traindf)\n",
    "    train_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "    test_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "    s=[]\n",
    "    for i in range(0,23792):\n",
    "        s.append(\" \")\n",
    "    test_df.drop(\"text\",axis=1,inplace=True)\n",
    "    test_df[\"text\"]=s\n",
    "    from keytotext import trainer\n",
    "    model=trainer()\n",
    "    model.from_pretrained(model_name=model_)\n",
    "    model.train(train_df=train_df, test_df=test_df,batch_size=5, max_epochs=2,use_gpu=True)\n",
    "    model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2f261",
   "metadata": {},
   "source": [
    "**The following 2  functions are to properly split the words into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findword(text):\n",
    "    sentendwords=[\".\",\",\",\"?\",\"!\",\";\"]\n",
    "    endwords={\".\":[],\",\":[],\"?\":[],\"!\":[],\";\":[]}\n",
    "    length=len(text)\n",
    "    for i in range(0,length):\n",
    "        if(text[i] in sentendwords):\n",
    "            j=i-length\n",
    "            while(j>=(0-length) and text[j]!=\" \"):\n",
    "                j-=1\n",
    "            endwords[text[i]].append(text[j+1:i].lower())\n",
    "    return endwords\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8699888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStops_andSplit(result):\n",
    "    global input_field\n",
    "    endwords=findword(input_field.get(1.0, \"end-1c\"))\n",
    "    dotwords=endwords[\".\"]\n",
    "    commawords=endwords[\",\"]\n",
    "    qwords=endwords[\"?\"]\n",
    "    semicolwords=endwords[\";\"]\n",
    "    excwords=endwords[\"!\"]\n",
    "    sentenders=endwords.keys()\n",
    "    networds=dotwords+commawords+qwords+semicolwords+excwords\n",
    "    splitted=result.split()\n",
    "    ends=[]\n",
    "    splitwords=[]\n",
    "    currlength=0\n",
    "    length=0\n",
    "    i=0\n",
    "    while(\"\" in networds):\n",
    "        networds.remove(\"\")\n",
    "    while(\" \" in networds):\n",
    "        networds.remove(\" \")\n",
    "    while(\"  \" in networds):\n",
    "        networds.remove(\"  \")\n",
    "    for word in splitted:\n",
    "        length+=(len(word)+1)\n",
    "        if(word in networds):\n",
    "            if (word in dotwords ):\n",
    "                ends.append(\".\")\n",
    "            elif (word in commawords):\n",
    "                ends.append(\",\")\n",
    "            elif (word in qwords ):\n",
    "                ends.append(\"?\")\n",
    "            elif (word in semicolwords ):\n",
    "                ends.append(\";\")\n",
    "            else :\n",
    "                ends.append(\"!\")\n",
    "            splitwords.append(result[currlength:length]+ends[i])\n",
    "            i+=1\n",
    "            currlength=length\n",
    "    if(currlength!=length):\n",
    "        splitwords.append(result[currlength:len(result)]+\".\")\n",
    "    return splitwords\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df59896",
   "metadata": {},
   "source": [
    "**predictK2T(testdf,modelpath)**\n",
    "\n",
    "will return summary predictions  for the test data frame uploaded using the model built from trainK2T and stored under modelpath. By default it is to be located under /model relative from current working directory, if doesnt, try to locate and give it as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictK2T(testdf,modelpath=\" \",gpu=True,onetime=True,new=False):\n",
    "    global model,input_field\n",
    "    if(new):\n",
    "        from keytotext import trainer\n",
    "        model=trainer()\n",
    "        model.load_model(modelpath, use_gpu=gpu)\n",
    "    kwords=testdf.iloc[:,1].values \n",
    "    predictions=[]\n",
    "    correct_sents=\"\"\n",
    "    for i in range(0,len(kwords)):\n",
    "        keywords=kwords[i]\n",
    "        result=model.predict(keywords.split(),use_gpu=gpu)\n",
    "        if(onetime):\n",
    "            newresult=createStops_andSplit(result)\n",
    "            for sents in newresult:\n",
    "                correct_sents+=(\" \".join(gf.correct(sents,max_candidates=1)))  \n",
    "        else:\n",
    "            correct_sents=gf.correct(result,max_candidates=1)\n",
    "        predictions.append(correct_sents)\n",
    "        if(onetime):\n",
    "            return predictions[0]\n",
    "    pred_df=pd.DataFrame()\n",
    "    pred_df[\"Predictions\"]=predictions\n",
    "    pred_df.to_csv(\"PredictionsResult.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalfunc():\n",
    "    global text_area\n",
    "    global input_field\n",
    "    global loading_label,gif\n",
    "    import pandas as pd\n",
    "    import tkinter as tk\n",
    "    gif=create_loading_gif()\n",
    "    loading_label.configure(image=gif)\n",
    "    s=input_field.get(1.0, \"end-1c\")\n",
    "    df=pd.DataFrame()\n",
    "    df[\"unused\"]=[0]\n",
    "    df[\"Article\"]=[s]\n",
    "    createCleaned(df,training=False,noFile=True)\n",
    "    df=pd.read_csv(\"Cleaned.csv\")\n",
    "    generateSet(df,training=False)\n",
    "    df=pd.read_csv(\"Keyword-text.csv\")\n",
    "    s=predictK2T(df,gpu=False)\n",
    "    text_area.delete(\"1.0\", \"end\")\n",
    "    text_area.insert(\"1.0\",s)\n",
    "    loading_label.configure(text=\"Generated!\",width=20, height=20, bg=\"grey\", font=(\"Arial\", 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd566526",
   "metadata": {},
   "source": [
    "### 4)GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03eeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_thread():\n",
    "    # create and start a new thread\n",
    "    thread = threading.Thread(target=finalfunc)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loading_gif():\n",
    "    global loading_gif\n",
    "    import tkinter as tk\n",
    "    # Create a PhotoImage object\n",
    "    loading_gif = tk.PhotoImage(file=\"loading.gif\")\n",
    "    # Return the PhotoImage object\n",
    "    return loading_gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateInterface():\n",
    "    import tkinter as tk\n",
    "    # create the root window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Text Summarizer\")\n",
    "    root.configure(bg=\"grey\")\n",
    "    root.state('zoomed')\n",
    "    # add three line breaks\n",
    "    for i in range(3):\n",
    "        tk.Label(root, text=\"\", bg=\"grey\").pack()\n",
    "    # create the heading label\n",
    "    heading = tk.Label(root, text=\"Abstractive Text Summarizer for News Articles\" , bg=\"grey\", font=(\"Arial\", 20))\n",
    "    heading.pack()\n",
    "    # add three line breaks\n",
    "    for i in range(3):\n",
    "        tk.Label(root, text=\"\", bg=\"grey\").pack()\n",
    "    # create a frame to hold the input field and text area\n",
    "    frame = tk.Frame(root)\n",
    "    frame.pack()\n",
    "    # create the input field\n",
    "    global input_field,text_area\n",
    "    input_field = tk.Text(frame, width=60, height=20, font=(\"Arial\", 12))\n",
    "    input_field.pack(side=\"left\")\n",
    "    # create the text area\n",
    "    text_area = tk.Label(frame, width=20, height=20, bg=\"grey\", font=(\"Arial\", 12))\n",
    "    text_area.pack(side=\"left\")\n",
    "    # create the text area\n",
    "    text_area = tk.Text(frame, width=60, height=20, font=(\"Arial\", 12))\n",
    "    text_area.pack(side=\"right\")\n",
    "    # add three line breaks\n",
    "    for i in range(3):\n",
    "        tk.Label(root, text=\"\", bg=\"grey\").pack()\n",
    "    # create the submit button\n",
    "    global submit_button\n",
    "    submit_button = tk.Button(root, text=\"Generate\", font=(\"Arial\", 14),command=start_thread)\n",
    "    submit_button.pack()\n",
    "    # add one line break\n",
    "    global loading_label\n",
    "    for i in range(1):\n",
    "        tk.Label(root, text=\"\", bg=\"grey\").pack()\n",
    "    loading_label = tk.Label(root, text=\"\",bg=\"grey\")\n",
    "    loading_label.pack()\n",
    "    # add three line breaks\n",
    "    for i in range(3):\n",
    "        tk.Label(root, text=\"\", bg=\"grey\").pack()\n",
    "    # create the subheading label\n",
    "    subheading = tk.Label(root, text=\"Created with <3 by Jagannathan\" , bg=\"grey\", font=(\"Arial\", 12))\n",
    "    subheading.pack()\n",
    "    # run the main loop\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2954b7e",
   "metadata": {},
   "source": [
    "### Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85173302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import os\n",
    "preImport(os.getcwd()+\"/model\")\n",
    "#Path under which the t5 keytotext model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "CreateInterface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
